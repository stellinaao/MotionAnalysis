{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67da1989",
   "metadata": {},
   "source": [
    "$\\text{\\large Dimensionality Reduction} \\\\ \n",
    "\\text{\\small Author: Stellina Ao (Churchland Lab)} \\newline\n",
    "\\text{\\small Date Created: 07/24/2025} \\newline\n",
    "\\text{\\small Last Edited: 07/24/2025} \\newline\n",
    "\\text{\\small Purpose: Compare T-SNE plots between young and old mice to check for differences in clusters} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa10604",
   "metadata": {},
   "source": [
    "Workflow\n",
    "1. pd --> 2d gaussian normalization [v]\n",
    "2. pca --> wvt []\n",
    "3. bandpass the result of wavelet decomp []\n",
    "4. tisne []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a39c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import scienceplots\n",
    "\n",
    "from scipy.stats import zscore, multivariate_normal\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from lib import constants, data, pca_utils, wvt_utils, tsne_utils\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39611de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty plots\n",
    "plt.style.use(['science','nature'])\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e369f",
   "metadata": {},
   "source": [
    "## Import and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea67f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and store in pd dataframe\n",
    "dlc_outputs = data.read_data()\n",
    "dlc_outputs = data.mean_imputation(dlc_outputs, 0.6) # impute the mean for uncertain vals (i.e., < 60% confidence) [must rectify by filtering out low freq bands intelligently]\n",
    "dlc_outputs = data.norm_session_bp(dlc_outputs) # normalize per session and per body part\n",
    "dlc_coords = data.get_coords(dlc_outputs)\n",
    "# norm_dict = data.get_norm_dict(dlc_outputs) <- for normalization across animals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd33805",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pca_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca first!\n",
    "p_ev = 0.9\n",
    "\n",
    "# determine number of components to use <- 1 component should be good enough! (indicates huge correlation between the body parts)\n",
    "evrs = pca_utils.get_pca_evr(dlc_coords, n_components=data.n_bodyparts*2, doPlot=True, n_rows=2, n_cols=4)\n",
    "n_pcs, max_n_pcs = pca_utils.get_num_pcs(evrs, p_ev)\n",
    "pca_utils.plot_cum_var(evrs, p_ev*100, n_pcs, n_rows=2, n_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = max_n_pcs\n",
    "\n",
    "dlc_reduxs = [PCA(n_components).fit_transform(dlc_coord) for dlc_coord in dlc_coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_utils.plot_pca(dlc_reduxs, n_rows=2, n_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c884ac1",
   "metadata": {},
   "source": [
    "## Wavelet Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wvt_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383adf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_in = np.logspace(-4, 4, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvt_decomp_outputs = [wvt_utils.wavelet_decomp_pca(dlc_reduxs[i], freqs_in) for i in range(data.n_subj)]\n",
    "cwtmatrs = [cwtmatr for (cwtmatr, freq) in wvt_decomp_outputs]\n",
    "freqs = [freq for (cwtmatr, freq) in wvt_decomp_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwtmatrs_fvec = [np.reshape(np.transpose(cwtmatr), (cwtmatr.shape[2], cwtmatr.shape[0]*cwtmatr.shape[1])) for cwtmatr in cwtmatrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e31249",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freq = np.shape(freqs)[2]\n",
    "n_fvec = n_components * (n_freq-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcms = wvt_utils.plot_wavelet_decomp_pca(cwtmatrs[0][:-1], freqs[0], n_rows=2, n_cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b703d2a",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# * upload to repo and pull on mike's desktop\n",
    "# * sanity check EVERYTHING (i.e., if i have to rerun this grid search because of some cs 2011 level bug i might combust)\n",
    "# * run my (presumably) several day long grid search T-T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2d002",
   "metadata": {},
   "source": [
    "### TSNE Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b831ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "embeds = []\n",
    "\n",
    "param_grid = ParameterGrid({'perplexity': np.logspace(start=2, stop=6, base=2, num=5), 'early_exaggeration': np.logspace(start=0, stop=4, base=2, num=5), 'learning_rate': np.logspace(start=1, stop=4, base=10, num=4)})\n",
    "max_iters = 5000\n",
    "n_iters_without_progress = 500\n",
    "n_jobs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07970ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def tsne(params, fvec):\n",
    "    with open(f\"tsne_gs_log.txt\", \"a\") as f:\n",
    "        print(f\"running tsne with params {params}\", file=f)\n",
    "\n",
    "    tsne = TSNE(n_components=3, max_iter=max_iters, n_iter_without_progress=n_iters_without_progress, n_jobs=n_jobs, perplexity=params['perplexity'], learning_rate=params['learning_rate'], early_exaggeration=params['early_exaggeration'])\n",
    "    embed = tsne.fit_transform(fvec)\n",
    "    return (tsne, embed)\n",
    "    \n",
    "results = Parallel(n_jobs=16)(delayed(tsne)(params, fvec) for params, fvec in zip(param_grid, cwtmatrs_fvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * grid search with the perplexity, early exaggeration, learning rate, bump up max_iters and n_iters w/o converging (n_jobs = -1, i.e., take all my processors, almighty sklearn)\n",
    "\n",
    "for params in param_grid:\n",
    "    model_params = []\n",
    "    embed_params = []\n",
    "    for i, fvec in enumerate(cwtmatrs_fvec):\n",
    "        tsne = TSNE(params)\n",
    "        embed_params.append(tsne.fit_transform(fvec))\n",
    "        model_params.append(tsne)\n",
    "    models.append(model_params)\n",
    "    embeds.append(embed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embed = [TSNE(n_components=3, perplexity=32).fit_transform(fvec) for fvec in cwtmatrs_fvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_flattened = np.array([coord for embed in tsne_embed for coord in embed])\n",
    "idx = np.array([l for ls in [i*np.ones(len(embed)) for i, embed in enumerate(tsne_embed)] for l in ls]).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40421a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(x=tsne_flattened[:,0], y=tsne_flattened[:,1], z=tsne_flattened[:,2], color=idx[:], title=\"T-SNE (All Subjects)\")\n",
    "\n",
    "fig.update_traces(marker=dict(size=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda70af",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26492d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = [KMeans().fit_predict(embed) for embed in tsne_embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aedd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tsne_embed)):\n",
    "    fig = px.scatter_3d(x=tsne_embed[i][:,0], y=tsne_embed[i][:,1], z=tsne_embed[i][:,2], color=k_means[i].astype('str'), title=data.subject_ids[i])\n",
    "\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69297f1",
   "metadata": {},
   "source": [
    "### Temporal Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad55a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tsne_embed)):\n",
    "    fig = px.scatter_3d(x=tsne_embed[i][:,0], y=tsne_embed[i][:,1], z=tsne_embed[i][:,2], color=np.arange(len(tsne_embed[i])), title=data.subject_ids[i])\n",
    "\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "    fig.update_layout(coloraxis_colorbar=dict(title=\"index\"))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235e2cd",
   "metadata": {},
   "source": [
    "### Probability Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657feb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsne_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b894c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_utils.plot_density_all(tsne_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot probability density and watershed analysis\n",
    "# ks test, plot cdf\n",
    "# normalize or not\n",
    "# check code\n",
    "\n",
    "# increase freq range, one animal one body part, no pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51973451",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance sampling technique\n",
    "# 1. generate an embedding using a selection of 600 data points from each subject out of 360k data points per subject [around 80k for us, 4.5 times less]\n",
    "# 2. t-SNE on 20k randomly selected data points from each subject\n",
    "# 3. embedding is used to estimate a probability density by convolving each point with a two-dimensional Gaussian whose width is equal to the distance from the point to its 10 nearest neighbours (N_embed)\n",
    "# 4. this space is segmented by applying a watershed transform to the inverse of the pdf, creating a set of regions\n",
    "# 5. finally, points are grouped by the region to which they belong and the number of points selected out of each region is proportional to the integral over the pdf in the region\n",
    "# 6. repeat for all datasets, yielding 35k data points in the training set\n",
    "\n",
    "def select_random_pts(data, n):\n",
    "    n_points = len(data)\n",
    "    \n",
    "    if n > n_points:\n",
    "        raise ValueError(f\"ERROR: the number of points to sample {n} can't be greater than the size of the pool {n_points}\")\n",
    "    return np.random.choice(data, n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab 10k random data points from each subject and embed with tsne (step 2)\n",
    "\n",
    "n_samples = 10000\n",
    "cwtmatrs_10k_sample = np.reshape([fvec[select_random_pts(np.arange(len(fvec)), n_samples)] for fvec in cwtmatrs_fvec], (data.n_subj*n_samples, n_fvec))\n",
    "tsne_10k = TSNE(n_components=3, perplexity=32).fit(cwtmatrs_10k_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_10k_embed = TSNE(n_components=3, perplexity=32).fit_transform(cwtmatrs_10k_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03962884",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(x=tsne_10k_embed[:10000,0], y=tsne_10k_embed[:10000,1], z=tsne_10k_embed[:10000,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87713afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding is used to estimate a probability density by convolving each point with a two-dimensional Gaussian whose width is equal to the distance from the point to its 10 nearest neighbours (N_embed) (step 3)\n",
    "def gaussian_kernel(n, width, normalized=False):\n",
    "    '''\n",
    "    Generates a n x n matrix with a centered gaussian \n",
    "    of standard deviation std centered on it. If normalized,\n",
    "    its volume equals 1.'''\n",
    "\n",
    "    std = width / (2*(2*np.log(2))**0.5)\n",
    "    gaussian1D = signal.gaussian(n, std)\n",
    "    gaussian2D = np.outer(gaussian1D, gaussian1D)\n",
    "    if normalized:\n",
    "        gaussian2D /= (2*np.pi*(std**2))\n",
    "    return gaussian2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f798008",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, perplexity=32)\n",
    "# then fit TSNE on the 35k sampled points and transform on every time point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
